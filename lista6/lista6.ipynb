{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "caa20513-ee7b-499b-890c-440626d0b3c3",
   "metadata": {},
   "source": [
    "Aprendizado de Máquina - 2024/3\n",
    "\n",
    "Aluno: Álvaro de Carvalho Alves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdff0e5f-4462-4492-82e9-3bce2a86feb5",
   "metadata": {},
   "source": [
    "# Questão 1\n",
    "\n",
    "Reinforcement Learning, Conceitos\n",
    "\n",
    "Explique a diferença entre Value iteration, Policy iteration e Q-learning. Para facilitar a explicação, mostre as equações usadas para cada caso."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e62cf34-a01e-4ed3-acba-38bc455ae09a",
   "metadata": {},
   "source": [
    "## **Resposta:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239fa9b9-c115-41da-8fe7-bc7a07850847",
   "metadata": {},
   "source": [
    "### Value Iteration: \n",
    "\n",
    "Calcula o valor ótimo de cada estado até convergir. A política ótima é então derivada desses valores ótimos.\n",
    "\n",
    "Equação:\n",
    "\n",
    "$$ V(s) = \\max_a \\left \\{ \\sum_{s'} P(s'|s,a) \\cdot \\left[ R(s,a,s') + \\gamma \\cdot V(s') \\right] \\right \\} $$\n",
    "\n",
    "Onde:\n",
    "\n",
    "$V(s)$ é o valor do estado $s$\n",
    "\n",
    "$a$ é uma ação possível\n",
    "\n",
    "$s'$ é um possível estado sucessor\n",
    "\n",
    "$P(s'|s,a)$ é a probabilidade de transição do estado $s$ para $s'$ ao tomar a ação $a$\n",
    "\n",
    "$R(s,a,s')$ é a recompensa recebida ao ir de $s$ para $s'$ com a ação $a$\n",
    "\n",
    "$\\gamma$ é o fator de desconto (quão importante são as recompensas futuras)\n",
    "\n",
    "\n",
    "### Policy Iteration\n",
    "\n",
    "Alterna entre a avaliação da política atual e a melhoria da política com base nessa avaliação.\n",
    "\n",
    "Equações:\n",
    "\n",
    "- Policy Evaluation:\n",
    "\n",
    "$$ V_\\pi(s) = \\sum_{s'} P(s'|s,\\pi(s)) \\cdot \\left[ R(s,\\pi(s),s') + \\gamma * V_\\pi(s') \\right] $$\n",
    "\n",
    "\n",
    "- Policy Improvement:\n",
    "\n",
    "$$ \\pi'(s) = argmax_a \\left \\{ \\sum_{s'} P(s'|s,a) \\cdot \\left [ R(s,a,s') + \\gamma \\cdot V_\\pi(s') \\right ] \\right \\} $$\n",
    "\n",
    "Onde:\n",
    "\n",
    "$V_\\pi(s)$ é o valor do estado $s$ sob a política $\\pi$\n",
    "\n",
    "$\\pi(s)$ é a ação que a política $\\pi$ dita no estado $s$\n",
    "\n",
    "$\\pi'(s)$ é a ação da política melhorada no estado $s$\n",
    "\n",
    "### Q-Learning\n",
    "\n",
    "Aprende diretamente a função $Q$, que representa o valor de tomar uma ação em um determinado estado. Não precisa de um modelo do ambiente.\n",
    "\n",
    "Equação:\n",
    "\n",
    "$$ Q(s,a) = Q(s,a) + \\alpha \\cdot \\left[ R(s,a,s') + \\gamma \\cdot \\max_{a'} Q(s',a') - Q(s,a) \\right] $$\n",
    "\n",
    "Onde:\n",
    "\n",
    "$Q(s,a)$ é o valor de tomar a ação $a$ no estado $s$\n",
    "\n",
    "$\\alpha$ é a taxa de aprendizado (quão rápido o algoritmo aprende com novas informações)\n",
    "\n",
    "$\\max_{a'} Q(s',a')$ é o valor estimado da melhor ação no próximo estado $s'$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea129751-afbb-4f3b-8437-9324294969f6",
   "metadata": {},
   "source": [
    "# Questão 2\n",
    "Otimização de um Sistema de Fila.\n",
    "\n",
    "Considere um sistema de fila onde clientes chegam e são atendidos em intervalos de tempo discretos (a cada slot) de tempo. O sistema é definido a seguir:\n",
    "\n",
    "1. Chegada de Clientes:\n",
    "    - No final de cada intervalo de tempo: 0, 2 ou 4 clientes chegam com probabilidades $p_0$, $p_2$ e $p_4$, respectivamente $(p_0 + p_2 + p_4 = 1)$.\n",
    "\n",
    "  \n",
    "2. Atendimento:\n",
    "    - No início de cada intervalo de tempo: Se houver $S$ servidores e $C$ clientes, o sistema atende $min(S, C)$ clientes.\n",
    "  \n",
    "    - Clientes restantes permanecem no sistemas (ficam para o próximo intervalo de tempo).\n",
    "\n",
    "3. Restrições do Sistema (para facilitar):\n",
    "    - Um máximo de 8 clientes pode estar no sistema em qualquer momento.\n",
    "    - O sistema inicia no estado inicial $(C, S) = (0,0)$, isto é, sem usuários e sem nenhum servidor.\n",
    "    \n",
    "4. Ações:\n",
    "    - No início de cada intervalo de tempo, pode-se:\n",
    "        - **-1:** Remover 1 servidor.\n",
    "        - **0:** Manter o número atual de servidores.\n",
    "        - **+1:** Adicionar 1 servidor.\n",
    "    - O número de servidores é limitado entre 1 e 3.\n",
    "\n",
    "5. Recompensas e Custos:\n",
    "    - **Ganhos:** $T$ por cliente atendido.\n",
    "    - **Custo do Servidor:** $R_s$ por servidor por intervalo de tempo.\n",
    "    - **Penalidade de Fila:** $R_q$ por slot, quando houver mais que 4 clientes no sistema.\n",
    "    - **Penalidade de Ociosidade:** $R_0$ por cada servidor não utilizado por intervalo de tempo.\n",
    "6. Objetivo:\n",
    "    - Projetar uma política $\\pi(s)$ que maximize a recompensa esperada:\n",
    "    - Você deve escolher um valor $\\gamma$ para o fator de desconto.\n",
    "  \n",
    "7. Você pode escolher entre value iteration, policy iteration, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982f1a05-2ee3-4f30-9ce0-008c1cce8b8f",
   "metadata": {},
   "source": [
    "## **RESPOSTA:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5baac345-0b62-4a45-995a-648c24e0486b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurações do problema\n",
    "MAX_CLIENTS = 8\n",
    "MIN_SERVERS = 1\n",
    "MAX_SERVERS = 3\n",
    "\n",
    "ARRIVAL_PROBS = {0: 0.4, 2: 0.2, 4: 0.4}  # p_0, p_2, p_4\n",
    "\n",
    "T = 10  # Ganho por cliente atendido\n",
    "R_S = 5  # Custo por servidor por intervalo\n",
    "R_Q = 10  # Penalidade por mais de 4 clientes na fila\n",
    "R_0 = 2  # Penalidade por servidor ocioso\n",
    "\n",
    "GAMMA = 0.9  # Fator de desconto\n",
    "EPSILON = 1e-3  # Critério de convergência"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b793874-aaea-4b20-9f43-7f06cd5db523",
   "metadata": {},
   "source": [
    "### Componentes do MDP\n",
    "Para o MDP, precisamos dos estados, ações, probabilidade de transições e recompensas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555a0392-bdb0-482f-95af-9f589f7cf982",
   "metadata": {},
   "source": [
    "\n",
    "#### Estados:\n",
    "Um estado do sistema é descrito por:\n",
    "\n",
    "$ s = (C, S) $\n",
    "\n",
    "Onde $C$ é o número de clientes no sistema ($0 \\leq C \\leq 8$) e $S$ é número de servidores no sistema ($1 \\leq S \\leq 3$).\n",
    "\n",
    "O espaço de estados tem tamanho $9 \\times 3 = 27$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9aeb6ac-40d4-4d81-9b1f-2db2b4b7edc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = [(C, S) for C in range(MAX_CLIENTS + 1) for S in range(MIN_SERVERS, MAX_SERVERS + 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89823072-e4a2-4914-9f12-189ee4735a2d",
   "metadata": {},
   "source": [
    "#### Ações:\n",
    "As ações possíveis são:\n",
    "\n",
    "$ a \\in \\{-1, 0, +1\\} $\n",
    "\n",
    "Que representam:\n",
    "\n",
    "- **Remover (-1):** Decrementar o número de servidores (mínimo 1).\n",
    "\n",
    "- **Manter (0):** Manter o número atual de servidores.\n",
    "\n",
    "- **Adicionar (+1):** Incrementar o número de servidores (até 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "295054a9-9c5d-4809-a007-89d2a3aa9332",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = [-1, 0, +1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3106f29-9a1f-43e6-b051-f63380b8eb4f",
   "metadata": {},
   "source": [
    "#### Transições:\n",
    "Os próximos estados dependem de:\n",
    "\n",
    "1. **Número de chegadas:** 0, 2, ou 4 clientes com probabilidades $p_0, p_2, p_4$.\n",
    "\n",
    "2. **Capacidade do sistema:** Número de clientes atendidos por $S$ servidores ($\\min(S, C)$).\n",
    "\n",
    "3. **Fila:** Os clientes não atendidos permanecem no sistema até o próximo intervalo.\n",
    "\n",
    "Dessa forma, dado um estado atual $(C, S)$ e uma ação $action$, um novo estado $(new\\_C, new\\_S)$ é computado por:\n",
    "\n",
    "$$ new\\_S = \\max(1, \\min(3, S + action)) $$\n",
    "$$ served\\_clients = \\min(S, C)$$\n",
    "$$ remaining\\_clients = C - served\\_clients $$\n",
    "$$ next\\_C = \\min(8, remaining\\_clients + arrivals) $$\n",
    "\n",
    "Para cada chegada $arrivals \\in {0, 2, 4}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd0be85c-c346-48dd-ab01-a9962e7b1478",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_next_states_and_probabilities(C, S, action):\n",
    "    \"\"\" Retorna os estados possíveis e suas probabilidades após a ação.\"\"\"\n",
    "    next_states = {}\n",
    "    new_S = max(MIN_SERVERS, min(MAX_SERVERS, S + action))  # Limita o número de servidores\n",
    "\n",
    "    for arrivals, prob in ARRIVAL_PROBS.items():\n",
    "        served_clients = min(S, C)  # Atende no máximo min(servidores, clientes)\n",
    "        remaining_clients = C - served_clients\n",
    "        next_C = min(MAX_CLIENTS, remaining_clients + arrivals)  # Aplica limite da fila\n",
    "\n",
    "        if (next_C, new_S) not in next_states:\n",
    "            next_states[(next_C, new_S)] = 0\n",
    "        next_states[(next_C, new_S)] += prob\n",
    "\n",
    "    return next_states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7253fdec-82a4-4d1c-a5b0-79d3f2ffa720",
   "metadata": {},
   "source": [
    "#### Recompensas:\n",
    "A recompensa depende de:\n",
    "\n",
    "1. **Clientes atendidos:** $T \\times \\text{número de clientes atendidos}$.\n",
    "\n",
    "2. **Custo dos servidores:** $R_s \\times S$.\n",
    "\n",
    "3. **Penalidade de fila cheia:** $R_q$ se $C > 4$.\n",
    "\n",
    "4. **Penalidade por ociosidade:** $R_0 \\times (S - \\min(S, C))$.\n",
    "\n",
    "A função de recompensa é:\n",
    "$$ R(s, a, s') = T \\times \\text{clientes atendidos} - R_s \\times S - R_q \\cdot \\mathbb{1}(C > 4) - R_0 \\cdot (S - \\text{clientes atendidos}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "942f5db7-eee6-4a30-9999-fea3cddc5f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reward(C, S, action, next_C, next_S):\n",
    "    \"\"\" Calcula a recompensa de transição para um estado específico.\"\"\"\n",
    "    served_clients = min(S, C)  # Clientes atendidos\n",
    "    idle_servers = S - served_clients\n",
    "\n",
    "    reward = T * served_clients  # Ganhos por clientes atendidos\n",
    "    reward -= R_S * S           # Custo por servidores\n",
    "\n",
    "    if next_C > 4:\n",
    "        reward -= R_Q           # Penalidade de fila cheia\n",
    "\n",
    "    reward -= R_0 * idle_servers  # Penalidade por servidores ociosos\n",
    "\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9feb53a-08cb-412a-afea-34fcdc685ea3",
   "metadata": {},
   "source": [
    "### 2.1. Value Iteration:\n",
    "\n",
    "1. Inicializar $V(s)$ arbitrariamente (aqui inicializo como 0).\n",
    "\n",
    "2. Para cada iteração $k$, atualize:\n",
    "$$ V_{k+1}(s) = \\max_a \\sum_{s'} P(s'|s, a) \\left[R(s, a, s') + \\gamma V_k(s')\\right] $$\n",
    "\n",
    "3. Parar quando convergir:\n",
    "$$ \\max_s |V_{k+1}(s) - V_k(s)| < \\epsilon $$\n",
    "\n",
    "4. Política resultante:\n",
    "$$ \\pi(s) = \\arg\\max_a \\sum_{s'} P(s'|s, a) \\left[R(s, a, s') + \\gamma V(s')\\right] $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c3b644b-ec7b-46b2-9df4-30249ebd9b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value Iteration\n",
    "V = {state: 0 for state in states}  # Inicialização dos valores dos estados\n",
    "policy = {state: 0 for state in states}  # Política inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "318c1cd1-d9d0-494a-b465-5f9c36438398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Política Ótima:\n",
      "Estado (0, 1): Ação 1\n",
      "Estado (0, 2): Ação 0\n",
      "Estado (0, 3): Ação -1\n",
      "Estado (1, 1): Ação 1\n",
      "Estado (1, 2): Ação 0\n",
      "Estado (1, 3): Ação -1\n",
      "Estado (2, 1): Ação -1\n",
      "Estado (2, 2): Ação 0\n",
      "Estado (2, 3): Ação -1\n",
      "Estado (3, 1): Ação 1\n",
      "Estado (3, 2): Ação -1\n",
      "Estado (3, 3): Ação -1\n",
      "Estado (4, 1): Ação 1\n",
      "Estado (4, 2): Ação 0\n",
      "Estado (4, 3): Ação -1\n",
      "Estado (5, 1): Ação 1\n",
      "Estado (5, 2): Ação 1\n",
      "Estado (5, 3): Ação -1\n",
      "Estado (6, 1): Ação 1\n",
      "Estado (6, 2): Ação 1\n",
      "Estado (6, 3): Ação 0\n",
      "Estado (7, 1): Ação 1\n",
      "Estado (7, 2): Ação 1\n",
      "Estado (7, 3): Ação 0\n",
      "Estado (8, 1): Ação 1\n",
      "Estado (8, 2): Ação 1\n",
      "Estado (8, 3): Ação 0\n",
      "\n",
      "Valores dos Estados:\n",
      "Estado (0, 1): Valor 21.33\n",
      "Estado (0, 2): Valor 14.33\n",
      "Estado (0, 3): Valor 7.33\n",
      "Estado (1, 1): Valor 33.33\n",
      "Estado (1, 2): Valor 26.33\n",
      "Estado (1, 3): Valor 19.33\n",
      "Estado (2, 1): Valor 34.86\n",
      "Estado (2, 2): Valor 38.33\n",
      "Estado (2, 3): Valor 31.33\n",
      "Estado (3, 1): Valor 40.20\n",
      "Estado (3, 2): Valor 39.86\n",
      "Estado (3, 3): Valor 43.33\n",
      "Estado (4, 1): Valor 38.47\n",
      "Estado (4, 2): Valor 45.20\n",
      "Estado (4, 3): Valor 44.82\n",
      "Estado (5, 1): Valor 40.64\n",
      "Estado (5, 2): Valor 47.70\n",
      "Estado (5, 3): Valor 50.20\n",
      "Estado (6, 1): Valor 37.17\n",
      "Estado (6, 2): Valor 47.96\n",
      "Estado (6, 3): Valor 52.70\n",
      "Estado (7, 1): Valor 37.36\n",
      "Estado (7, 2): Valor 45.94\n",
      "Estado (7, 3): Valor 52.96\n",
      "Estado (8, 1): Valor 36.64\n",
      "Estado (8, 2): Valor 46.48\n",
      "Estado (8, 3): Valor 50.94\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    delta = 0  # Diferença máxima entre iterações\n",
    "    new_V = V.copy()\n",
    "\n",
    "    for state in states:\n",
    "        C, S = state\n",
    "        best_value = float('-inf')\n",
    "        best_action = 0\n",
    "\n",
    "        for action in actions:\n",
    "            # Calcular o valor esperado da ação\n",
    "            value = 0\n",
    "            next_states = get_next_states_and_probabilities(C, S, action)\n",
    "\n",
    "            for (next_C, next_S), prob in next_states.items():\n",
    "                reward = get_reward(C, S, action, next_C, next_S)\n",
    "                value += prob * (reward + GAMMA * V[(next_C, next_S)])\n",
    "\n",
    "            # Atualiza a melhor ação\n",
    "            if value > best_value:\n",
    "                best_value = value\n",
    "                best_action = action\n",
    "\n",
    "        # Atualiza o valor do estado\n",
    "        new_V[state] = best_value\n",
    "        policy[state] = best_action\n",
    "\n",
    "        # Calcula a mudança máxima\n",
    "        delta = max(delta, abs(new_V[state] - V[state]))\n",
    "\n",
    "    V = new_V\n",
    "\n",
    "    # Critério de convergência\n",
    "    if delta < EPSILON:\n",
    "        break\n",
    "\n",
    "# Exibir resultados finais\n",
    "print(\"Política Ótima:\")\n",
    "for state in sorted(policy):\n",
    "    print(f\"Estado {state}: Ação {policy[state]}\")\n",
    "\n",
    "print(\"\\nValores dos Estados:\")\n",
    "for state in sorted(V):\n",
    "    print(f\"Estado {state}: Valor {V[state]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6abd81a-f022-4b1f-8074-38058af0c84f",
   "metadata": {},
   "source": [
    "### Simulação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac679d74-58fa-446f-b68e-2ef26932aa9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Simulação de Transições:\n",
      "Passo 1: Estado (0, 1) -> Ação 1 -> Recompensa -7.00 -> Próximo Estado (4, 2)\n",
      "Passo 2: Estado (4, 2) -> Ação 0 -> Recompensa 10.00 -> Próximo Estado (2, 2)\n",
      "Passo 3: Estado (2, 2) -> Ação 0 -> Recompensa 10.00 -> Próximo Estado (4, 2)\n",
      "Passo 4: Estado (4, 2) -> Ação 0 -> Recompensa 10.00 -> Próximo Estado (4, 2)\n",
      "Passo 5: Estado (4, 2) -> Ação 0 -> Recompensa 0.00 -> Próximo Estado (6, 2)\n",
      "Passo 6: Estado (6, 2) -> Ação 1 -> Recompensa 0.00 -> Próximo Estado (8, 3)\n",
      "Passo 7: Estado (8, 3) -> Ação 0 -> Recompensa 5.00 -> Próximo Estado (8, 3)\n",
      "Passo 8: Estado (8, 3) -> Ação 0 -> Recompensa 5.00 -> Próximo Estado (8, 3)\n",
      "Passo 9: Estado (8, 3) -> Ação 0 -> Recompensa 5.00 -> Próximo Estado (8, 3)\n",
      "Passo 10: Estado (8, 3) -> Ação 0 -> Recompensa 5.00 -> Próximo Estado (7, 3)\n",
      "Passo 11: Estado (7, 3) -> Ação 0 -> Recompensa 5.00 -> Próximo Estado (8, 3)\n",
      "Passo 12: Estado (8, 3) -> Ação 0 -> Recompensa 5.00 -> Próximo Estado (7, 3)\n",
      "Passo 13: Estado (7, 3) -> Ação 0 -> Recompensa 5.00 -> Próximo Estado (6, 3)\n",
      "Passo 14: Estado (6, 3) -> Ação 0 -> Recompensa 15.00 -> Próximo Estado (3, 3)\n",
      "Passo 15: Estado (3, 3) -> Ação -1 -> Recompensa 15.00 -> Próximo Estado (2, 2)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def simulate(policy, steps=10):\n",
    "    \"\"\" Simula as transições no sistema de filas usando a política ótima.\"\"\"\n",
    "    state = (0, MIN_SERVERS)  # Estado inicial\n",
    "    history = []\n",
    "\n",
    "    for _ in range(steps):\n",
    "        C, S = state\n",
    "        action = policy[state]  # Ação baseada na política\n",
    "\n",
    "        # Determinar próximo estado\n",
    "        next_states = get_next_states_and_probabilities(C, S, action)\n",
    "        next_state = random.choices(list(next_states.keys()), weights=next_states.values())[0]\n",
    "\n",
    "        # Calcular recompensa\n",
    "        reward = get_reward(C, S, action, *next_state)\n",
    "\n",
    "        # Registrar histórico\n",
    "        history.append((state, action, reward, next_state))\n",
    "\n",
    "        # Atualizar estado\n",
    "        state = next_state\n",
    "\n",
    "    return history\n",
    "\n",
    "# Executar simulação\n",
    "simulation_history = simulate(policy, steps=15)\n",
    "\n",
    "print(\"\\nSimulação de Transições:\")\n",
    "for i, (state, action, reward, next_state) in enumerate(simulation_history):\n",
    "    print(f\"Passo {i+1}: Estado {state} -> Ação {action} -> Recompensa {reward:.2f} -> Próximo Estado {next_state}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
